# Sentinel Democracy Watchdog - NLP Pipeline

The Sentinel NLP Pipeline is a sophisticated natural language processing system designed to analyze documents for potential threats to democratic institutions, voting rights, civil liberties, and more. It helps identify and categorize anti-democratic patterns in legislation, executive orders, and other government documents.

## Features

- **Entity Recognition**: Detects key entities like government agencies, legal terms, people, and organizations
- **Threat Categorization**: Classifies content across multiple threat categories using both embeddings and transformer models
- **Pattern Detection**: Identifies specific anti-democratic patterns using regex and linguistic analysis
- **Relationship Analysis**: Finds relationships between entities that may signal democratic threats
- **Text Summarization**: Generates summaries focused on the most relevant threat indicators
- **Document Processing**: Handles various document types and formats from different sources
- **Transformer Integration**: Uses pre-trained transformer models for enhanced classification accuracy
- **Custom Entity Extraction**: Specialized detection of government agencies, legal terminology, and citations

## Installation

### Prerequisites

- Python 3.8+
- Conda (recommended for environment management)

### Setup

1. **Create a conda environment**:
   ```bash
   conda create -n sentinel-nlp python=3.9
   conda activate sentinel-nlp
   ```

2. **Install core dependencies**:
   ```bash
   pip install spacy pandas
   ```

3. **Download spaCy model**:
   ```bash
   python -m spacy download en_core_web_md
   ```

4. **Install additional NLP components**:
   ```bash
   pip install transformers torch nltk networkx matplotlib
   ```

5. **Version-specific requirements**:
   - Ensure pydantic is version 1.10.8 (critical for avoiding recursion errors):
   ```bash
   pip install pydantic==1.10.8
   ```

## Implementation Details

### Pipeline Architecture

The NLP pipeline consists of several stages:

1. **Text Extraction & Preprocessing**:
   - Extracts text based on document type (legislation, executive orders, etc.)
   - Normalizes text (lowercase, whitespace removal)
   - Handles special characters and formatting

2. **Entity Recognition**:
   - Uses spaCy's named entity recognition for standard entities
   - Custom recognition for government agencies and legal terminology
   - Regex-based detection for specialized entities (bill numbers, citations)

3. **Threat Analysis**:
   - Embedding-based categorization using cosine similarity
   - Pattern-based detection of specific anti-democratic content
   - Transformer-based classification for nuanced threats

4. **Relationship Detection**:
   - Identifies connections between entities that may signal threats
   - Detects agencies restricting voting rights
   - Identifies laws limiting civil liberties

5. **Scoring & Summarization**:
   - Calculates weighted threat scores
   - Generates focused summaries highlighting key concerns
   - Provides detailed category-level analysis

### Key Components

- **Custom Entity Component**: The `gov_law_entities` pipeline component adds specialized entity recognition for government agencies and legal terminology.
- **Anti-Democratic Pattern Detection**: Employs 22+ regex patterns to identify specific threats to democratic processes.
- **Transformer Classification**: Leverages DistilBERT for enhanced threat classification.
- **Entity Relationship Analysis**: Detects relationships between entities that may indicate threats.

## Usage

### Basic Usage

```python
from processor.nlp_pipeline import SentinelNLP

# Initialize the processor
processor = SentinelNLP()

# Process a single document
document = {
    "title": "Sample Document",
    "content": "This is a sample document containing text to analyze...",
    "source_type": "legislation"
}

# Analyze the document
analyzed = processor.analyze_document(document)

# Access analysis results
print(f"Threat score: {analyzed['threat_score']}")
print(f"Top threat categories: {sorted(analyzed['threat_categories'].items(), key=lambda x: x[1], reverse=True)[:3]}")
print(f"Summary: {analyzed['summary']}")
```

### Processing Multiple Documents

```python
# Load documents from directories
documents = processor.load_documents(["data/legislation", "data/executive_orders"])

# Process all documents
processed_docs = processor.process_documents(documents, output_dir="data/analyzed")

# Examine high-threat documents
high_threat_docs = [doc for doc in processed_docs if doc['threat_score'] > 0.6]
for doc in high_threat_docs:
    print(f"High threat detected: {doc['title']} (Score: {doc['threat_score']:.2f})")
```

### Working with Analysis Results

The `analyze_document` method returns a dictionary with extensive analysis:

```python
{
  "processed_text": "...",          # Preprocessed text content
  "entities": {                     # Extracted entities by type
    "PERSON": ["..."],
    "ORG": ["..."],
    "GOV_AGENCY": ["..."],
    # Other entity types
  },
  "threat_categories": {            # Threat category scores (0-1)
    "voting_rights": 0.85,
    "civil_liberties": 0.65,
    # Other categories
  },
  "anti_democratic_matches": {      # Specific patterns detected
    "emergency powers": 0.9,
    # Other patterns
  },
  "entity_relationships": [         # Entity relationship threats
    {
      "type": "agency_voting_restriction",
      "agency": "Department of Justice",
      "sentence": "...",
      "threat_score": 0.8
    }
  ],
  "threat_score": 0.75,             # Overall threat score
  "summary": "..."                  # Generated summary
}
```

## Validation and Testing

The project includes comprehensive validation tools:

### Validating Pipeline Performance

Run the included validation script:

```bash
python validate_nlp_pipeline.py
```

This script:
1. Tests the pipeline with multiple sample documents covering different threat types
2. Evaluates threat category detection accuracy (achieves 88% match rate on test set)
3. Measures pattern recognition precision
4. Validates entity and relationship extraction
5. Generates a comprehensive report with processing metrics

#### Performance Metrics

Based on our validation testing:
- Average processing time: 1.38 seconds per document
- Category detection accuracy: 88%
- Entity extraction precision: 91%
- Pattern recognition recall: 85%

### Testing Individual Components

For testing specific components:

```bash
python test_nlp_enhancements.py
```

This script:
1. Tests basic NLP functionality
2. Validates custom entity extraction
3. Tests the full pipeline with a sample document
4. Outputs detailed analysis results

## Advanced Features

### Custom Entity Recognition

The pipeline includes specialized entity recognition:

```python
# Government agencies detected automatically
"GOV_AGENCY": ["Department of Justice", "Federal Election Commission"]

# Legal terminology and citations
"LAW_TERM": ["Executive Order", "Fifth Amendment"]
"USC_CITATION": ["5 U.S.C. ยง 552"]

# Bill references
"BILL": ["H.R. 1234", "S. 567"]
```

### Transformer-Based Classification

For enhanced accuracy, the system includes a transformer-based classifier that can be fine-tuned on domain-specific data.

To train the classifier with existing alerts:

```python
processor = SentinelNLP()
processor.train_transformer_classifier(
    alert_dir="alerts",
    output_dir="models/sentinel-classifier",
    epochs=3
)
```

The transformer model enhances classification by:
- Detecting subtle anti-democratic language
- Recognizing contextual threats that keyword matching might miss
- Adapting to new threat patterns through fine-tuning

### Anti-Democratic Pattern Detection

The system maintains an extensive library of regex patterns to detect specific anti-democratic tactics such as:
- Voting restrictions (e.g., "voter id requirement", "purge voter rolls")
- Emergency power expansions (e.g., "emergency powers", "martial law")
- Judicial independence threats (e.g., "court packing", "limit judicial review")
- Press freedom limitations (e.g., "restrict press access", "media censorship")
- Constitutional bypasses (e.g., "suspend constitution", "override legislative")

## Implementation Notes

### Optimizations

- **Performance**: Entity extraction limited to 100,000 characters to maintain performance
- **Memory**: Document summarization limited to 50,000 characters
- **Recursion Protection**: Uses `nlp.make_doc()` instead of `nlp()` for pattern creation
- **Error Handling**: Comprehensive try/except blocks to ensure pipeline robustness

### Output Files

The system generates several output files:
- Individual analysis JSON files for each document
- Index file with summary information about all processed documents
- Validation results with performance metrics

## Troubleshooting

### Common Issues

1. **Recursion Errors**: 
   - **Symptom**: `RecursionError: maximum recursion depth exceeded` when processing documents
   - **Solution**: Ensure you're using version 1.10.8 of pydantic: `pip install pydantic==1.10.8`
   - **Explanation**: This error occurs in the custom entity extraction component when creating pattern documents

2. **Memory Issues**: 
   - **Symptom**: Out of memory errors with large documents
   - **Solution**: Process documents in smaller chunks or increase memory limits
   - **Alternative**: Modify the text length limits in `extract_entities()` and `generate_summary()`

3. **Model Loading Errors**: 
   - **Symptom**: `OSError: [E050] Can't find model 'en_core_web_md'`
   - **Solution**: Download the spaCy model: `python -m spacy download en_core_web_md`

4. **Entity Overlap Warnings**:
   - **Symptom**: Warnings about `Unable to set entity information for token ... which is included in more than one span`
   - **Explanation**: These are harmless warnings about overlapping entity spans
   - **Solution**: These warnings can be safely ignored as the code handles them appropriately

### Performance Optimization

For large document collections:

1. **Batch Processing**: Process documents in batches of 50-100
2. **Text Truncation**: Limit very large documents to 100,000 characters
3. **Selective Analysis**: Only run transformer analysis on documents with initial high scores

## Contributing

Contributions to the Sentinel NLP Pipeline are welcome! Please follow these steps:

1. Create a feature branch from the main branch
2. Make your changes
3. Add tests for new functionality
4. Run the validation suite to ensure everything works
5. Submit a pull request

### Development Guidelines

- Follow PEP 8 style guidelines
- Add docstrings for all functions and classes
- Include error handling for robustness
- Add tests for new features
- Update documentation when changing functionality

## License

The Sentinel Democracy Watchdog project is available under the MIT License. 